{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945bf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing necessary dependences:\n",
    "#!pip install -q keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c16806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "deb975f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "EnvironmentNameNotFound: Could not find conda environment: tf-gpu\n",
      "You can list all discoverable environments with `conda info --envs`.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e3cb425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, MultiHeadAttention, Softmax, TextVectorization, LayerNormalization\n",
    "from keras import Model, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1df3a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14473d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff6f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54d52dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Data\n",
    "filepath = os.path.dirname(os.path.dirname(os.path.abspath(\".\"))) + \"/Datasets\"\n",
    "filepath = re.sub(r'\\\\', '/', filepath)\n",
    "print(filepath)\n",
    "X1 = pd.read_csv(filepath + \"/Tatobea Project\" + '/'+ \"spa\" +'.txt', sep='\\t', header = None)[[0,1]].rename(columns = {0:\"English\", 1:\"Translated\"})\n",
    "X1 = X1[X1.notnull().all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7af5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  English  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Go.   \n",
      "2                                                     Go.   \n",
      "3                                                     Go.   \n",
      "4                                                     Hi.   \n",
      "...                                                   ...   \n",
      "139008  A carbon footprint is the amount of carbon dio...   \n",
      "139009  Since there are usually multiple websites on a...   \n",
      "139010  If you want to sound like a native speaker, yo...   \n",
      "139011  It may be impossible to get a completely error...   \n",
      "139012  One day, I woke up to find that God had put ha...   \n",
      "\n",
      "                                               Translated  \n",
      "0                                                     Ve.  \n",
      "1                                                   Vete.  \n",
      "2                                                   Vaya.  \n",
      "3                                                 Váyase.  \n",
      "4                                                   Hola.  \n",
      "...                                                   ...  \n",
      "139008  Una huella de carbono es la cantidad de contam...  \n",
      "139009  Como suele haber varias páginas web sobre cual...  \n",
      "139010  Si quieres sonar como un hablante nativo, debe...  \n",
      "139011  Puede que sea imposible obtener un corpus comp...  \n",
      "139012  Un día, me desperté y vi que Dios me había pue...  \n",
      "\n",
      "[139013 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8188b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1['Translated'] = \"<ES> \" + X1['Translated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47e98e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  English  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Go.   \n",
      "2                                                     Go.   \n",
      "3                                                     Go.   \n",
      "4                                                     Hi.   \n",
      "...                                                   ...   \n",
      "139008  A carbon footprint is the amount of carbon dio...   \n",
      "139009  Since there are usually multiple websites on a...   \n",
      "139010  If you want to sound like a native speaker, yo...   \n",
      "139011  It may be impossible to get a completely error...   \n",
      "139012  One day, I woke up to find that God had put ha...   \n",
      "\n",
      "                                               Translated  \n",
      "0                                                <ES> Ve.  \n",
      "1                                              <ES> Vete.  \n",
      "2                                              <ES> Vaya.  \n",
      "3                                            <ES> Váyase.  \n",
      "4                                              <ES> Hola.  \n",
      "...                                                   ...  \n",
      "139008  <ES> Una huella de carbono es la cantidad de c...  \n",
      "139009  <ES> Como suele haber varias páginas web sobre...  \n",
      "139010  <ES> Si quieres sonar como un hablante nativo,...  \n",
      "139011  <ES> Puede que sea imposible obtener un corpus...  \n",
      "139012  <ES> Un día, me desperté y vi que Dios me habí...  \n",
      "\n",
      "[139013 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc30d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\n",
      "DGT-TM\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN -NL\n",
      "EN -NL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-BG\n",
      "\\EN-BG\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-CS\n",
      "\\EN-CS\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-DA\n",
      "\\EN-DA\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-DE\n",
      "\\EN-DE\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-EL\n",
      "\\EN-EL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-ES\n",
      "\\EN-ES\n",
      "EN\n",
      "ES\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-ES/EN-ES.txt\n",
      "                                             English  \\\n",
      "0            Commission Regulation (EC) No 1788/2004   \n",
      "1                                 of 15 October 2004   \n",
      "2  fixing the minimum selling prices for butter f...   \n",
      "3        THE COMMISSION OF THE EUROPEAN COMMUNITIES,   \n",
      "4  Having regard to the Treaty establishing the E...   \n",
      "\n",
      "                                          Translated  \n",
      "0   <ES> Reglamento (CE) no 1788/2004 de la Comisión  \n",
      "1                      <ES> de 15 de octubre de 2004  \n",
      "2  <ES> por el que se fijan los precios mínimos d...  \n",
      "3      <ES> LA COMISIÓN DE LAS COMUNIDADES EUROPEAS,  \n",
      "4  <ES> Visto el Tratado constitutivo de la Comun...  \n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-ET\n",
      "\\EN-ET\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-FI\n",
      "\\EN-FI\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-FR\n",
      "\\EN-FR\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-GA\n",
      "\\EN-GA\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-HR\n",
      "\\EN-HR\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-HU\n",
      "\\EN-HU\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-IT\n",
      "\\EN-IT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-LT\n",
      "\\EN-LT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-LV\n",
      "\\EN-LV\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-MT\n",
      "\\EN-MT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-PL\n",
      "\\EN-PL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-PT\n",
      "\\EN-PT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-RO\n",
      "\\EN-RO\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-SK\n",
      "\\EN-SK\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-SL\n",
      "\\EN-SL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-SV\n",
      "\\EN-SV\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\Sample TMX file (EN-GA)\n",
      "EN-GA)\n"
     ]
    }
   ],
   "source": [
    "for d in [x[0] for x in os.walk(filepath + \"/DGT-TM\")]:\n",
    "    print(d)\n",
    "    print(d[-6:])\n",
    "    #Ignores TMX files\n",
    "    if d[-1] == \")\" or d[-6:] == \"DGT-TM\":\n",
    "        continue\n",
    "    #This code actually works, however due to the dataset it takes 1+ hour to run.\n",
    "    #Because of this, at least for now I'll be restricting it to just the EN-ES folder.\n",
    "    te = True\n",
    "    if d[-5:] != \"EN-ES\":\n",
    "        te = False\n",
    "        continue\n",
    "    \n",
    "    fp = re.sub(r'\\\\', '/', d)\n",
    "    E = fp.split(\"/\")[-1].split(\"-\")[0]\n",
    "    print(E)\n",
    "    T = fp.split(\"/\")[-1].split(\"-\")[1]\n",
    "    print(T)\n",
    "    print(d + '/'+ E + \"-\" + T +'.txt')\n",
    "    \n",
    "    temp = pd.read_csv(d + '/'+ E + \"-\" + T +'.txt', sep='\\t', header = None)[[0,1]].rename(columns = {0:\"English\", 1:\"Translated\"})\n",
    "    temp = temp[temp.notnull().all(axis = 1)]\n",
    "    temp['Translated'] = \"<\" + T + \"> \" + temp['Translated']\n",
    "    \n",
    "    print(temp.head())\n",
    "    if te:\n",
    "        X1 = pd.concat([X1, temp])\n",
    "    else:\n",
    "        train_x = pd.concat([train_x, temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb52bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = pd.DataFrame()\n",
    "Y1['Translated'] = X1.pop(\"Translated\")\n",
    "Y1['Translated'] = \"[start] \" + Y1['Translated'] + \" [end]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "643752f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Translated\n",
      "0                                   [start] <ES> Ve. [end]\n",
      "1                                 [start] <ES> Vete. [end]\n",
      "2                                 [start] <ES> Vaya. [end]\n",
      "3                               [start] <ES> Váyase. [end]\n",
      "4                                 [start] <ES> Hola. [end]\n",
      "...                                                    ...\n",
      "5696845         [start] <ES> Código país tercero [1] [end]\n",
      "5696846     [start] <ES> Valor global de importación [end]\n",
      "5696847  [start] <ES> Nomenclatura de países fijada por...\n",
      "5696848  [start] <ES> El código «999» significa «otros ...\n",
      "5696849  [start] <ES> DO L 337 de 24.12.1994, p. 66; Re...\n",
      "\n",
      "[5835639 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6f2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1, test_x, train_y1, test_y = train_test_split(X1, Y1, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a995787",
   "metadata": {},
   "outputs": [],
   "source": [
    "#val_x, test_x, val_y, test_y = train_test_split(test_x, test_y, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "592b1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_y = train_x.pop(\"Translated\")\n",
    "except:\n",
    "    #train_x doesn't have any portions, so we'll just describe it as a normal Dataframe\n",
    "    train_y = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "616f89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.concat([train_x, train_x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a1975bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.concat([train_y, train_y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "148fc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x1\n",
    "del train_y1\n",
    "del X1\n",
    "del Y1\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17bf6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   English\n",
      "4569925  Repair and maintenance services of other profe...\n",
      "5159865  If the policy-holder belongs to a group of und...\n",
      "1200393                       Cash flow (% of total sales)\n",
      "3355922                                 White mineral oils\n",
      "3934638  Directive 2004/17/EC of the European Parliamen...\n",
      "...                                                    ...\n",
      "1431032                                         Division D\n",
      "2095551                 Done at Brussels, 7 December 2012.\n",
      "4787669  The project's strategy is grounded in SEESAC's...\n",
      "4165720  In S.11.01 the 15th paragraph, second bullet o...\n",
      "1553774                                           Whereas:\n",
      "\n",
      "[2917819 rows x 1 columns]\n",
      "                                                Translated\n",
      "4569925  [start] <ES> Servicios de reparación y manteni...\n",
      "5159865  [start] <ES> Si el tomador del seguro formara ...\n",
      "1200393  [start] <ES> Flujo de caja (% de las ventas to...\n",
      "3355922       [start] <ES> Aceites minerales blancos [end]\n",
      "3934638  [start] <ES> Directiva 2004/17/CE del Parlamen...\n",
      "...                                                    ...\n",
      "1431032                      [start] <ES> División D [end]\n",
      "2095551  [start] <ES> Hecho en Bruselas, el 7 de diciem...\n",
      "4787669  [start] <ES> La estrategia del proyecto se fun...\n",
      "4165720  [start] <ES> en S.11.01, el párrafo decimoquin...\n",
      "1553774      [start] <ES> Considerando lo siguiente: [end]\n",
      "\n",
      "[2917819 rows x 1 columns]\n",
      "                                                   English\n",
      "1874101  The measures provided for in this Decision are...\n",
      "3088268  Having regard to the opinion of the European P...\n",
      "1671913    For the Commission, On behalf of the President,\n",
      "2092155  32012 R 0086: Commission Implementing Regulati...\n",
      "4214123  Direct links have been established between exp...\n",
      "...                                                    ...\n",
      "3256664  Further contact and correspondence followed pe...\n",
      "482859   If cheese is mentioned together with other ing...\n",
      "1492322                           Subject matter and scope\n",
      "3685568  Deadline for submitting the particulars of the...\n",
      "4183040  The approval periods of the active substances ...\n",
      "\n",
      "[2917820 rows x 1 columns]\n",
      "                                                Translated\n",
      "1874101  [start] <ES> Las medidas previstas en la prese...\n",
      "3088268  [start] <ES> Visto el dictamen del Parlamento ...\n",
      "1671913  [start] <ES> Por la Comisión, en nombre del Pr...\n",
      "2092155  [start] <ES> 32012 R 0086: Reglamento de Ejecu...\n",
      "4214123  [start] <ES> Se han establecido vínculos direc...\n",
      "...                                                    ...\n",
      "3256664  [start] <ES> Se mantuvieron nuevos contactos y...\n",
      "482859   [start] <ES> Si el queso aparece junto con otr...\n",
      "1492322   [start] <ES> Objeto y ámbito de aplicación [end]\n",
      "3685568  [start] <ES> Plazo de presentación de los dato...\n",
      "4183040  [start] <ES> El Reglamento (UE) n.o 823/2012 d...\n",
      "\n",
      "[2917820 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_x)\n",
    "print(train_y)\n",
    "print(test_x)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7348904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917819, 1)\n",
      "(2917819, 1)\n",
      "(2917820, 1)\n",
      "(2917820, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a8f5f087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Repair and maintenance services of other professional electrical equipment', '[start] <ES> Servicios de reparación y mantenimiento de otros equipos eléctricos profesionales [end]']\n",
      "['If the policy-holder belongs to a group of undertakings for which consolidated accounts within the meaning of Directive 83/349/EEC (7) are drawn up, the criteria mentioned above shall be applied on the basis of the consolidated accounts.', '[start] <ES> Si el tomador del seguro formara parte de un conjunto de empresas cuyo balance consolidado se establece con arreglo a lo dispuesto en la Directiva 83/349/CEE (7), los criterios mencionados anteriormente se aplicarán sobre la base del balance consolidado. [end]']\n"
     ]
    }
   ],
   "source": [
    "train_pairs = pd.concat([train_x['English'], train_y['Translated']], axis=1).values.tolist()\n",
    "val_pairs = pd.concat([test_x['English'], test_y['Translated']], axis=1).values.tolist()\n",
    "print(train_pairs[0])\n",
    "print(train_pairs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8b90bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed458f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x.astype(str)\n",
    "#test_x = test_x.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8963c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.concat([train_x, test_x])\n",
    "#Y = pd.concat([train_y, test_y])\n",
    "#Y_SP = Y[Y['Translated'].str.contains(\"<ES>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c0ee9049",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('engvocab.txt', 'r', encoding=\"utf-8\")\n",
    "input_vocab = file.read().split(\"\\n\")\n",
    "file.close()\n",
    "input_vocab_size = len(input_vocab)\n",
    "\n",
    "file = open('spavocab.txt', 'r', encoding=\"utf-8\")\n",
    "output_vocab = file.read().split(\"\\n\")\n",
    "file.close()\n",
    "output_vocab_size = len(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8abf9687",
   "metadata": {},
   "outputs": [],
   "source": [
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "batch_size = 64\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(lowercase, \"[%s]\" % re.escape(strip_chars), \"\")\n",
    "\n",
    "\n",
    "eng_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "spa_vectorization = TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_eng_texts = [pair[0] for pair in train_pairs]\n",
    "train_spa_texts = [pair[1] for pair in train_pairs]\n",
    "#eng_vectorization.adapt(train_eng_texts)\n",
    "#spa_vectorization.adapt(train_spa_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a9ba3f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_dataset(eng, spa):\n",
    "    eng = eng_vectorization(eng)\n",
    "    spa = spa_vectorization(spa)\n",
    "    return (\n",
    "        {\n",
    "            \"encoder_inputs\": eng,\n",
    "            \"decoder_inputs\": spa[:, :-1],\n",
    "        },\n",
    "        spa[:, 1:],\n",
    "    )\n",
    "\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "38273a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bcfd3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual Model:\n",
    "#For now, let's start by creating an exact copy of the attention is all you need model.\n",
    "#I tried for a long time to make a model using only the Keras Functional API,\n",
    "#but it seems like I will need to create some custom layers in order to succeed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4fc127e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(\n",
    "  d_model, # Input/output dimensionality.\n",
    "  dff # Inner-layer dimensionality.\n",
    "  ):\n",
    "\n",
    "    return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
    "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "20696428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*,\n",
    "               d_model, # Input/output dimensionality.\n",
    "               num_attention_heads,\n",
    "               dff, # Inner-layer dimensionality.\n",
    "               dropout_rate=0.1\n",
    "               ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.embed_dim = d_model\n",
    "        self.dense_dim = dff\n",
    "        self.num_heads = num_attention_heads\n",
    "        \n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        self.dense_proj = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\n",
    "        # Dropout for the point-wise feed-forward network.\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "\n",
    "        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eafdb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "               *,\n",
    "               d_model, # Input/output dimensionality.\n",
    "               num_attention_heads,\n",
    "               dff, # Inner-layer dimensionality.\n",
    "               dropout_rate=0.1\n",
    "               ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked multi-head self-attention.\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=d_model, # Size of each attention head for query Q and key K.\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        # Multi-head cross-attention.\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=d_model, # Size of each attention head for query Q and key K.\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Point-wise feed-forward network.\n",
    "        self.dense_proj = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # Layer normalization.\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.add = tf.keras.layers.Add()  # instead of `+` to preserve mask\n",
    "\n",
    "        # Dropout for the point-wise feed-forward network.\n",
    "        #self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        # The encoder output shape is `(batch_size, input_seq_len, d_model)`.\n",
    "\n",
    "    \n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n",
    "        )\n",
    "        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(self.add([out_2, proj_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "830bae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original hyperparameters\n",
    "num_layers = 4\n",
    "embed_dim = 256\n",
    "latent_dim = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "epochs=50\n",
    "batch_size = 128\n",
    "sequence_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e6890a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset_train = tf.data.Dataset.from_tensor_slices((x_train_tensor, y_train_tensor)).batch(batch_size)\n",
    "#Dataset_test = tf.data.Dataset.from_tensor_slices((x_test_tensor, y_test_tensor)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80cb5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d08d984e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mDataset_train\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(Dataset_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "#print(Dataset_train)\n",
    "#print(Dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00825243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, num_layers, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.enc_layers = [\n",
    "        EncoderLayer(\n",
    "          d_model=embed_dim,\n",
    "          num_attention_heads=num_heads,\n",
    "          dff=dense_dim,\n",
    "          dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        for i in range(self.num_layers):\n",
    "            #x = self.enc_layers[i](x, mask)\n",
    "            x = self.enc_layers[i](x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f7efcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "523da844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, num_layers,  dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dec_layers = [\n",
    "        DecoderLayer(\n",
    "          d_model=embed_dim,\n",
    "          num_attention_heads=num_heads,\n",
    "          dff=latent_dim,\n",
    "          dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        x = inputs\n",
    "        for i in range(self.num_layers):\n",
    "            #x = self.dec_layers[i](x, mask)\n",
    "            x = self.dec_layers[i](x, encoder_outputs)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c437cb1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15001\n"
     ]
    }
   ],
   "source": [
    "print(input_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9c66f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, input_vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads, num_layers, dropout_rate)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, output_vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads, num_layers, dropout_rate)(x, encoded_seq_inputs)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = tf.keras.layers.Dense(output_vocab_size, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ede4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "497e6120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " positional_embedding_4 (Positi  (None, None, 256)   3905792     ['encoder_inputs[0][0]']         \n",
      " onalEmbedding)                                                                                   \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " transformer_encoder_1 (Transfo  (None, None, 256)   9469952     ['positional_embedding_4[0][0]'] \n",
      " rmerEncoder)                                                                                     \n",
      "                                                                                                  \n",
      " model_2 (Functional)           (None, None, 15001)  25647257    ['decoder_inputs[0][0]',         \n",
      "                                                                  'transformer_encoder_1[0][0]']  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 39,023,001\n",
      "Trainable params: 39,023,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "170cce7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "    4/45591 [..............................] - ETA: 16:33:13 - loss: 4.3241 - accuracy: 0.0822"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/GPU:1\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_ds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\engine\\training.py:1564\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[0;32m   1557\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1558\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1561\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   1562\u001b[0m ):\n\u001b[0;32m   1563\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1564\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1565\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[0;32m   1566\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:947\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    945\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    946\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 947\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateless_fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    949\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    950\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    951\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1863\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/GPU:1'):\n",
    "    transformer.fit(train_ds, epochs=epochs, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0e6a1d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6995546f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "e2d9a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "1fea87b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_built_with_cuda())\n",
    "print(tf.test.is_gpu_available(cuda_only=False, min_cuda_compute_capability=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c78883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = Input(shape=(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = open(\"textvectorvocap.txt\", \"r\", encoding=\"utf-8\")\n",
    "#m = r.readline().split(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TLayer = TextVectorization(output_sequence_length = max_padding_size, vocabulary = m, output_mode = \"int\")\n",
    "#inp = TextVectorization(output_sequence_length = max_padding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d52056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp = TLayer(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c040a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp.adapt(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee294dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inp.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b917cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"textvectorvocap.txt\", \"w\", encoding=\"utf-8\")\n",
    "#f.write(\"*\".join(inp.get_vocabulary()))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff44ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = open(\"textvectorvocap.txt\", \"r\", encoding=\"utf-8\")\n",
    "#m = r.readline().split(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(m == inp.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e06a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inp.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0637f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x_vec = inp.predict(train_x)\n",
    "#test_x_vec = inp.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(i.__call__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#em = Embedding(input_dim = max_length, output_dim = 512)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5361145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#posen = keras_nlp.layers.SinePositionEncoding()(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mh1 = MultiHeadAttention(num_heads=3, key_dim=512, value_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mho1 = mh1(posen, posen, posen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no1 = LayerNormalization()\n",
    "#ou1 = no1(mho1+posen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fe1 = Dense(512)(ou1)\n",
    "#ou2 = no1(ou1+fe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lin = Dense(tar_vocab_length)(ou2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb22ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = Softmax()(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09597290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=i, outputs=s)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to figure out metrics for language translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
