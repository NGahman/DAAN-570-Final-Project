{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "945bf1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Installing necessary dependences:\n",
    "#!pip install -q keras-nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6c16806",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb975f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3cb425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports:\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as tf_text\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.layers import Embedding, Dense, MultiHeadAttention, Softmax, TextVectorization, LayerNormalization\n",
    "from keras import Model, Input\n",
    "from keras.models import Sequential\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from tensorflow_text.tools.wordpiece_vocab import bert_vocab_from_dataset as bert_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14473d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend\n"
     ]
    }
   ],
   "source": [
    "import keras_nlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bff6f9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e54d52dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing Data\n",
    "filepath = os.path.dirname(os.path.dirname(os.path.abspath(\".\"))) + \"/Datasets\"\n",
    "filepath = re.sub(r'\\\\', '/', filepath)\n",
    "print(filepath)\n",
    "X1 = pd.read_csv(filepath + \"/Tatobea Project\" + '/'+ \"spa\" +'.txt', sep='\\t', header = None)[[0,1]].rename(columns = {0:\"English\", 1:\"Translated\"})\n",
    "X1 = X1[X1.notnull().all(axis = 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7af5c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  English  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Go.   \n",
      "2                                                     Go.   \n",
      "3                                                     Go.   \n",
      "4                                                     Hi.   \n",
      "...                                                   ...   \n",
      "139008  A carbon footprint is the amount of carbon dio...   \n",
      "139009  Since there are usually multiple websites on a...   \n",
      "139010  If you want to sound like a native speaker, yo...   \n",
      "139011  It may be impossible to get a completely error...   \n",
      "139012  One day, I woke up to find that God had put ha...   \n",
      "\n",
      "                                               Translated  \n",
      "0                                                     Ve.  \n",
      "1                                                   Vete.  \n",
      "2                                                   Vaya.  \n",
      "3                                                 Váyase.  \n",
      "4                                                   Hola.  \n",
      "...                                                   ...  \n",
      "139008  Una huella de carbono es la cantidad de contam...  \n",
      "139009  Como suele haber varias páginas web sobre cual...  \n",
      "139010  Si quieres sonar como un hablante nativo, debe...  \n",
      "139011  Puede que sea imposible obtener un corpus comp...  \n",
      "139012  Un día, me desperté y vi que Dios me había pue...  \n",
      "\n",
      "[139013 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8188b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1['Translated'] = \"<ES> \" + X1['Translated']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47e98e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  English  \\\n",
      "0                                                     Go.   \n",
      "1                                                     Go.   \n",
      "2                                                     Go.   \n",
      "3                                                     Go.   \n",
      "4                                                     Hi.   \n",
      "...                                                   ...   \n",
      "139008  A carbon footprint is the amount of carbon dio...   \n",
      "139009  Since there are usually multiple websites on a...   \n",
      "139010  If you want to sound like a native speaker, yo...   \n",
      "139011  It may be impossible to get a completely error...   \n",
      "139012  One day, I woke up to find that God had put ha...   \n",
      "\n",
      "                                               Translated  \n",
      "0                                                <ES> Ve.  \n",
      "1                                              <ES> Vete.  \n",
      "2                                              <ES> Vaya.  \n",
      "3                                            <ES> Váyase.  \n",
      "4                                              <ES> Hola.  \n",
      "...                                                   ...  \n",
      "139008  <ES> Una huella de carbono es la cantidad de c...  \n",
      "139009  <ES> Como suele haber varias páginas web sobre...  \n",
      "139010  <ES> Si quieres sonar como un hablante nativo,...  \n",
      "139011  <ES> Puede que sea imposible obtener un corpus...  \n",
      "139012  <ES> Un día, me desperté y vi que Dios me habí...  \n",
      "\n",
      "[139013 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06c447aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6cc30d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\n",
      "DGT-TM\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN -NL\n",
      "EN -NL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-BG\n",
      "\\EN-BG\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-CS\n",
      "\\EN-CS\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-DA\n",
      "\\EN-DA\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-DE\n",
      "\\EN-DE\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-EL\n",
      "\\EN-EL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-ES\n",
      "\\EN-ES\n",
      "EN\n",
      "ES\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-ES/EN-ES.txt\n",
      "                                             English  \\\n",
      "0            Commission Regulation (EC) No 1788/2004   \n",
      "1                                 of 15 October 2004   \n",
      "2  fixing the minimum selling prices for butter f...   \n",
      "3        THE COMMISSION OF THE EUROPEAN COMMUNITIES,   \n",
      "4  Having regard to the Treaty establishing the E...   \n",
      "\n",
      "                                          Translated  \n",
      "0   <ES> Reglamento (CE) no 1788/2004 de la Comisión  \n",
      "1                      <ES> de 15 de octubre de 2004  \n",
      "2  <ES> por el que se fijan los precios mínimos d...  \n",
      "3      <ES> LA COMISIÓN DE LAS COMUNIDADES EUROPEAS,  \n",
      "4  <ES> Visto el Tratado constitutivo de la Comun...  \n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-ET\n",
      "\\EN-ET\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-FI\n",
      "\\EN-FI\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-FR\n",
      "\\EN-FR\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-GA\n",
      "\\EN-GA\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-HR\n",
      "\\EN-HR\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-HU\n",
      "\\EN-HU\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-IT\n",
      "\\EN-IT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-LT\n",
      "\\EN-LT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-LV\n",
      "\\EN-LV\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-MT\n",
      "\\EN-MT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-PL\n",
      "\\EN-PL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-PT\n",
      "\\EN-PT\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-RO\n",
      "\\EN-RO\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-SK\n",
      "\\EN-SK\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-SL\n",
      "\\EN-SL\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\EN-SV\n",
      "\\EN-SV\n",
      "C:/Users/nickg/OneDrive/Documents/DAAN 570/Group Project/Datasets/DGT-TM\\Sample TMX file (EN-GA)\n",
      "EN-GA)\n"
     ]
    }
   ],
   "source": [
    "for d in [x[0] for x in os.walk(filepath + \"/DGT-TM\")]:\n",
    "    print(d)\n",
    "    print(d[-6:])\n",
    "    #Ignores TMX files\n",
    "    if d[-1] == \")\" or d[-6:] == \"DGT-TM\":\n",
    "        continue\n",
    "    #This code actually works, however due to the dataset it takes 1+ hour to run.\n",
    "    #Because of this, at least for now I'll be restricting it to just the EN-ES folder.\n",
    "    te = True\n",
    "    if d[-5:] != \"EN-ES\":\n",
    "        te = False\n",
    "        continue\n",
    "    \n",
    "    fp = re.sub(r'\\\\', '/', d)\n",
    "    E = fp.split(\"/\")[-1].split(\"-\")[0]\n",
    "    print(E)\n",
    "    T = fp.split(\"/\")[-1].split(\"-\")[1]\n",
    "    print(T)\n",
    "    print(d + '/'+ E + \"-\" + T +'.txt')\n",
    "    \n",
    "    temp = pd.read_csv(d + '/'+ E + \"-\" + T +'.txt', sep='\\t', header = None)[[0,1]].rename(columns = {0:\"English\", 1:\"Translated\"})\n",
    "    temp = temp[temp.notnull().all(axis = 1)]\n",
    "    temp['Translated'] = \"<\" + T + \"> \" + temp['Translated']\n",
    "    \n",
    "    print(temp.head())\n",
    "    if te:\n",
    "        X1 = pd.concat([X1, temp])\n",
    "    else:\n",
    "        train_x = pd.concat([train_x, temp])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb52bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y1 = pd.DataFrame()\n",
    "Y1['Translated'] = X1.pop(\"Translated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "643752f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Translated\n",
      "0                                                 <ES> Ve.\n",
      "1                                               <ES> Vete.\n",
      "2                                               <ES> Vaya.\n",
      "3                                             <ES> Váyase.\n",
      "4                                               <ES> Hola.\n",
      "...                                                    ...\n",
      "5696845                       <ES> Código país tercero [1]\n",
      "5696846                   <ES> Valor global de importación\n",
      "5696847  <ES> Nomenclatura de países fijada por el Regl...\n",
      "5696848   <ES> El código «999» significa «otros orígenes».\n",
      "5696849  <ES> DO L 337 de 24.12.1994, p. 66; Reglamento...\n",
      "\n",
      "[5835639 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f6f2ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x1, test_x, train_y1, test_y = train_test_split(X1, Y1, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a995787",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "592b1324",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    train_y = train_x.pop(\"Translated\")\n",
    "except:\n",
    "    #train_x doesn't have any portions, so we'll just describe it as a normal Dataframe\n",
    "    train_y = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "616f89e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = pd.concat([train_x, train_x1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a1975bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = pd.concat([train_y, train_y1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "148fc0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x1\n",
    "del train_y1\n",
    "del X1\n",
    "del Y1\n",
    "del temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "17bf6479",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   English\n",
      "4569925  Repair and maintenance services of other profe...\n",
      "5159865  If the policy-holder belongs to a group of und...\n",
      "1200393                       Cash flow (% of total sales)\n",
      "3355922                                 White mineral oils\n",
      "3934638  Directive 2004/17/EC of the European Parliamen...\n",
      "...                                                    ...\n",
      "1431032                                         Division D\n",
      "2095551                 Done at Brussels, 7 December 2012.\n",
      "4787669  The project's strategy is grounded in SEESAC's...\n",
      "4165720  In S.11.01 the 15th paragraph, second bullet o...\n",
      "1553774                                           Whereas:\n",
      "\n",
      "[2917819 rows x 1 columns]\n",
      "                                                Translated\n",
      "4569925  <ES> Servicios de reparación y mantenimiento d...\n",
      "5159865  <ES> Si el tomador del seguro formara parte de...\n",
      "1200393       <ES> Flujo de caja (% de las ventas totales)\n",
      "3355922                     <ES> Aceites minerales blancos\n",
      "3934638  <ES> Directiva 2004/17/CE del Parlamento Europ...\n",
      "...                                                    ...\n",
      "1431032                                    <ES> División D\n",
      "2095551  <ES> Hecho en Bruselas, el 7 de diciembre de 2...\n",
      "4787669  <ES> La estrategia del proyecto se fundamenta ...\n",
      "4165720  <ES> en S.11.01, el párrafo decimoquinto, segu...\n",
      "1553774                    <ES> Considerando lo siguiente:\n",
      "\n",
      "[2917819 rows x 1 columns]\n",
      "                                                   English\n",
      "1874101  The measures provided for in this Decision are...\n",
      "3088268  Having regard to the opinion of the European P...\n",
      "1671913    For the Commission, On behalf of the President,\n",
      "2092155  32012 R 0086: Commission Implementing Regulati...\n",
      "4214123  Direct links have been established between exp...\n",
      "...                                                    ...\n",
      "3256664  Further contact and correspondence followed pe...\n",
      "482859   If cheese is mentioned together with other ing...\n",
      "1492322                           Subject matter and scope\n",
      "3685568  Deadline for submitting the particulars of the...\n",
      "4183040  The approval periods of the active substances ...\n",
      "\n",
      "[2917820 rows x 1 columns]\n",
      "                                                Translated\n",
      "1874101  <ES> Las medidas previstas en la presente Deci...\n",
      "3088268     <ES> Visto el dictamen del Parlamento Europeo,\n",
      "1671913     <ES> Por la Comisión, en nombre del Presidente\n",
      "2092155  <ES> 32012 R 0086: Reglamento de Ejecución (UE...\n",
      "4214123  <ES> Se han establecido vínculos directos entr...\n",
      "...                                                    ...\n",
      "3256664  <ES> Se mantuvieron nuevos contactos y corresp...\n",
      "482859   <ES> Si el queso aparece junto con otros ingre...\n",
      "1492322                 <ES> Objeto y ámbito de aplicación\n",
      "3685568  <ES> Plazo de presentación de los datos de una...\n",
      "4183040  <ES> El Reglamento (UE) n.o 823/2012 de la Com...\n",
      "\n",
      "[2917820 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "print(train_x)\n",
    "print(train_y)\n",
    "print(test_x)\n",
    "print(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c7348904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2917819, 1)\n",
      "(2917819, 1)\n",
      "(2917820, 1)\n",
      "(2917820, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape)\n",
    "print(train_y.shape)\n",
    "print(test_x.shape)\n",
    "print(test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ed458f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x = train_x.astype(str)\n",
    "#test_x = test_x.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8963c8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X = pd.concat([train_x, test_x])\n",
    "#Y = pd.concat([train_y, test_y])\n",
    "#Y_SP = Y[Y['Translated'].str.contains(\"<ES>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abf9687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6e82d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_padding_series_org = X['English'].str.split(\" \")\n",
    "#max_padding_len_org = max_padding_series_org.str.len()\n",
    "#print(max_padding_len_org)\n",
    "#max_padding_size_org = max_padding_len_org.max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0b56a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The +2 is necessary because of the [START] and [END] tokens\n",
    "#print(max_padding_size_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ef7e1129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_padding_series_tar = Y_SP['Translated'].str.split(\" \")\n",
    "#max_padding_len_tar = max_padding_series_tar.str.len()\n",
    "#print(max_padding_len_tar)\n",
    "#max_padding_size_tar = max_padding_len_tar.max() + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d2eab129",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(max_padding_size_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "71446d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code used to find the length of \n",
    "#vectorizer_org = CountVectorizer()\n",
    "#count_org = vectorizer_org.fit_transform(X['English'])\n",
    "#vectorizer_tar = CountVectorizer()\n",
    "#count_tar = vectorizer_tar.fit_transform(Y_SP['Translated'])\n",
    "#print(count_org, count_tar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fabfd2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(count_org.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ddce75c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(vectorizer_org.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "534bdd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(vectorizer_tar.get_feature_names_out()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a3541b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The total number of words in the English corpus seems to be 285636.\n",
    "#The total number of words in the Spanish corpus seems to be 340903.\n",
    "org_vocab_length = 285636\n",
    "tar_vocab_length = 340903"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "353d5c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to tokenize train_x, test_x, train_y, test_y\n",
    "#This code creates the vocabulary and saves it to a text file.\n",
    "bert_tokenizer_params=dict(lower_case=True)\n",
    "reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[START]\", \"[END]\"]\n",
    "\n",
    "bert_vocab_org_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = org_vocab_length,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")\n",
    "\n",
    "bert_vocab_tar_args = dict(\n",
    "    # The target vocabulary size\n",
    "    vocab_size = tar_vocab_length,\n",
    "    # Reserved tokens that must be included in the vocabulary\n",
    "    reserved_tokens=reserved_tokens,\n",
    "    # Arguments for `text.BertTokenizer`\n",
    "    bert_tokenizer_params=bert_tokenizer_params,\n",
    "    # Arguments for `wordpiece_vocab.wordpiece_tokenizer_learner_lib.learn`\n",
    "    learn_params={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f11d55f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower_case': True}\n"
     ]
    }
   ],
   "source": [
    "print(bert_tokenizer_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f0e4a62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_tensor = tf.convert_to_tensor(X['English'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d52f1964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "12817687",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_dataset = tf.data.Dataset.from_tensor_slices(X_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "caea7f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#en_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#    X_dataset.batch(1000).prefetch(2),\n",
    "#    **bert_vocab_org_args\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ad920763",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"engvocab.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "#    for token in en_vocab:\n",
    "#      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0275f37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_SP_tensor = tf.convert_to_tensor(Y_SP['Translated'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6979af06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_SP_dataset = tf.data.Dataset.from_tensor_slices(Y_SP_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6f7e01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#es_vocab = bert_vocab.bert_vocab_from_dataset(\n",
    "#    Y_SP_dataset.batch(1000).prefetch(2),\n",
    "#    **bert_vocab_tar_args\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36806668",
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open(\"esvocab.txt\", 'w', encoding=\"utf-8\") as f:\n",
    "#    for token in es_vocab:\n",
    "#      print(token, file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e541881c",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('engvocab.txt', 'r', encoding=\"utf-8\")\n",
    "input_vocab = len(file.read().split(\"\\n\"))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d1ceaf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('esvocab.txt', 'r', encoding=\"utf-8\")\n",
    "output_vocab = len(file.read().split(\"\\n\"))\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dc015052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86320 100347\n"
     ]
    }
   ],
   "source": [
    "print(input_vocab, output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ddc43b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "es_tokenizer = tf_text.BertTokenizer('esvocab.txt', **bert_tokenizer_params)\n",
    "en_tokenizer = tf_text.BertTokenizer('engvocab.txt', **bert_tokenizer_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fff2c400",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "max_length = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "7a66819d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tensor = en_tokenizer.tokenize(train_x['English'])\n",
    "x_train_tensor = x_train_tensor.merge_dims(-2,-1)\n",
    "#Max length of tensor needs to be truncated due to OOM issues\n",
    "x_train_tensor = x_train_tensor[:,:max_length]\n",
    "x_train_tensor = x_train_tensor.to_tensor(default_value=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "442bc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#x_train_tensor = tf.slice(x_train_tensor, [0,0], (x_train_tensor.shape[0], max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6f7e5afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97ba416a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_tensor = en_tokenizer.tokenize(test_x['English'])\n",
    "x_test_tensor = x_test_tensor.merge_dims(-2,-1)\n",
    "x_test_tensor = x_test_tensor[:,:max_length]\n",
    "x_test_tensor = x_test_tensor.to_tensor(default_value=0)\n",
    "#x_test_tensor = tf.slice(x_test_tensor, [0,0], (x_test_tensor.shape[0], max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d464da92",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "42bda269",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_tensor = es_tokenizer.tokenize(train_y['Translated'])\n",
    "y_train_tensor = y_train_tensor.merge_dims(-2,-1)\n",
    "y_train_tensor = y_train_tensor[:,:max_length]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cbd6aa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 is the ID for the [PAD] token\n",
    "y_train_tensor = y_train_tensor.to_tensor(default_value=0)\n",
    "#y_train_tensor = tf.slice(y_train_tensor, [0,0], (y_train_tensor.shape[0], max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7e5289c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b0317d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor = es_tokenizer.tokenize(test_y['Translated'])\n",
    "y_test_tensor = y_test_tensor.merge_dims(-2,-1)\n",
    "y_test_tensor = y_test_tensor[:,:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ad66ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_tensor = y_test_tensor.to_tensor(default_value=0)\n",
    "#y_test_tensor = tf.slice(y_test_tensor, [0,0], (y_test_tensor.shape[0], max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fba00121",
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ff697e27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CountVectorizer', 'Dense', 'E', 'Embedding', 'In', 'Input', 'LayerNormalization', 'Model', 'MultiHeadAttention', 'Out', 'Sequential', 'Softmax', 'T', 'TextVectorization', '_', '__', '___', '__builtin__', '__builtins__', '__doc__', '__loader__', '__name__', '__package__', '__spec__', '_dh', '_i', '_i1', '_i10', '_i11', '_i12', '_i13', '_i14', '_i15', '_i16', '_i17', '_i18', '_i19', '_i2', '_i20', '_i21', '_i22', '_i23', '_i24', '_i25', '_i26', '_i27', '_i28', '_i29', '_i3', '_i30', '_i31', '_i32', '_i33', '_i34', '_i35', '_i36', '_i37', '_i38', '_i39', '_i4', '_i40', '_i41', '_i42', '_i43', '_i44', '_i45', '_i46', '_i47', '_i48', '_i49', '_i5', '_i50', '_i51', '_i52', '_i53', '_i54', '_i55', '_i56', '_i57', '_i58', '_i59', '_i6', '_i7', '_i8', '_i9', '_ih', '_ii', '_iii', '_oh', 'bert_tokenizer_params', 'bert_vocab', 'bert_vocab_org_args', 'bert_vocab_tar_args', 'd', 'en_tokenizer', 'es_tokenizer', 'exit', 'file', 'filepath', 'fp', 'gc', 'get_ipython', 'input_vocab', 'keras', 'keras_nlp', 'max_length', 'np', 'open', 'org_vocab_length', 'os', 'output_vocab', 'pd', 'quit', 're', 'reserved_tokens', 'tar_vocab_length', 'te', 'tf', 'tf_text', 'train_test_split', 'x_test_tensor', 'x_train_tensor', 'y_test_tensor', 'y_train_tensor']\n"
     ]
    }
   ],
   "source": [
    "print(dir())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dcb8b737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n",
      "(None, None)\n",
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_train_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f26836b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6ff14918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(None,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5a642509",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(2917819, 611)\n",
    "#(2917820, 777)\n",
    "#(2917819, 622)\n",
    "#(2917820, 794)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b8ba9f57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#org_length = 777\n",
    "#tar_length = 794\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22b8241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if x_train_tensor.shape[1] < org_length:\n",
    "#    x_train_tensor = tf.concat([x_train_tensor, tf.zeroes((x_train_tensor[0], (org_length - x_train_tensor.shape[1])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b9f9dd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if x_test_tensor.shape[1] < org_length:\n",
    "#    x_test_tensor = tf.concat([x_test_tensor, tf.zeroes((x_test_tensor[0], (org_length - x_test_tensor.shape[1])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e3a43b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if y_train_tensor.shape[1] < tar_length:\n",
    "#    y_train_tensor = tf.concat([y_train_tensor, tf.zeroes((y_train_tensor[0], (tar_length - y_train_tensor.shape[1])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "27dd840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if y_test_tensor.shape[1] < tar_length:\n",
    "#    y_test_tensor = tf.concat([y_test_tensor, tf.zeroes((y_test_tensor[0], (tar_length - y_test_tensor.shape[1])))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a4eaa1b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, None)\n",
      "(None, None)\n",
      "(None, None)\n",
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "print(x_train_tensor.shape)\n",
    "print(x_test_tensor.shape)\n",
    "print(y_train_tensor.shape)\n",
    "print(y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "38273a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#STOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "bcfd3aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual Model:\n",
    "#For now, let's start by creating an exact copy of the attention is all you need model.\n",
    "#I tried for a long time to make a model using only the Keras Functional API,\n",
    "#but it seems like I will need to create some custom layers in order to succeed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4fc127e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward_network(\n",
    "  d_model, # Input/output dimensionality.\n",
    "  dff # Inner-layer dimensionality.\n",
    "  ):\n",
    "\n",
    "  return tf.keras.Sequential([\n",
    "      tf.keras.layers.Dense(dff, activation='relu'),  # Shape `(batch_size, seq_len, dff)`.\n",
    "      tf.keras.layers.Dense(d_model)  # Shape `(batch_size, seq_len, d_model)`.\n",
    "  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "20696428",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,*,\n",
    "               d_model, # Input/output dimensionality.\n",
    "               num_attention_heads,\n",
    "               dff, # Inner-layer dimensionality.\n",
    "               dropout_rate=0.1\n",
    "               ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "\n",
    "        self.embed_dim = d_model\n",
    "        self.dense_dim = dff\n",
    "        self.num_heads = num_attention_heads\n",
    "        \n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim\n",
    "        )\n",
    "        \n",
    "        self.dense_proj = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "\n",
    "\n",
    "        # Dropout for the point-wise feed-forward network.\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "\n",
    "\n",
    "        attention_output = self.attention(query=inputs, value=inputs, key=inputs)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "eafdb8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self,\n",
    "               *,\n",
    "               d_model, # Input/output dimensionality.\n",
    "               num_attention_heads,\n",
    "               dff, # Inner-layer dimensionality.\n",
    "               dropout_rate=0.1\n",
    "               ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked multi-head self-attention.\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=d_model, # Size of each attention head for query Q and key K.\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "        # Multi-head cross-attention.\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(\n",
    "            num_heads=num_attention_heads,\n",
    "            key_dim=d_model, # Size of each attention head for query Q and key K.\n",
    "            dropout=dropout_rate\n",
    "        )\n",
    "\n",
    "        # Point-wise feed-forward network.\n",
    "        self.dense_proj = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        # Layer normalization.\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.add = tf.keras.layers.Add()  # instead of `+` to preserve mask\n",
    "\n",
    "        # Dropout for the point-wise feed-forward network.\n",
    "        #self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        # The encoder output shape is `(batch_size, input_seq_len, d_model)`.\n",
    "\n",
    "    \n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs, value=inputs, key=inputs, use_causal_mask=True\n",
    "        )\n",
    "        out_1 = self.layernorm_1(self.add([inputs, attention_output_1]))\n",
    "\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=out_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "        )\n",
    "        out_2 = self.layernorm_2(self.add([out_1, attention_output_2]))\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(self.add([out_2, proj_output]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "830bae31",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Original hyperparameters\n",
    "num_layers = 4\n",
    "embed_dim = 256\n",
    "latent_dim = 512\n",
    "num_heads = 8\n",
    "dropout_rate = 0.1\n",
    "epochs=50\n",
    "batch_size = 128\n",
    "sequence_length = max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e6890a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset_train = tf.data.Dataset.from_tensor_slices((x_train_tensor, y_train_tensor)).batch(batch_size)\n",
    "Dataset_test = tf.data.Dataset.from_tensor_slices((x_test_tensor, y_test_tensor)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "80cb5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d08d984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>\n",
      "<BatchDataset element_spec=(TensorSpec(shape=(None, None), dtype=tf.int64, name=None), TensorSpec(shape=(None, None), dtype=tf.int64, name=None))>\n"
     ]
    }
   ],
   "source": [
    "print(Dataset_train)\n",
    "print(Dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "00825243",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, num_layers, dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.enc_layers = [\n",
    "        EncoderLayer(\n",
    "          d_model=embed_dim,\n",
    "          num_attention_heads=num_heads,\n",
    "          dff=dense_dim,\n",
    "          dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        x = inputs\n",
    "        for i in range(self.num_layers):\n",
    "            #x = self.enc_layers[i](x, mask)\n",
    "            x = self.enc_layers[i](x)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"dense_dim\": self.dense_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "f7efcb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(tf.keras.layers.Layer):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = Embedding(\n",
    "            input_dim=vocab_size, output_dim=embed_dim\n",
    "        )\n",
    "        self.position_embeddings = Embedding(\n",
    "            input_dim=sequence_length, output_dim=embed_dim\n",
    "        )\n",
    "        self.sequence_length = sequence_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embed_dim = embed_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"sequence_length\": self.sequence_length,\n",
    "                \"vocab_size\": self.vocab_size,\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "            }\n",
    "        )\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "523da844",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, num_layers,  dropout_rate, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.dec_layers = [\n",
    "        DecoderLayer(\n",
    "          d_model=embed_dim,\n",
    "          num_attention_heads=num_heads,\n",
    "          dff=latent_dim,\n",
    "          dropout_rate=dropout_rate)\n",
    "        for _ in range(num_layers)]\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        x = inputs\n",
    "        for i in range(self.num_layers):\n",
    "            #x = self.dec_layers[i](x, mask)\n",
    "            x = self.dec_layers[i](x, encoder_outputs)\n",
    "        return x\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update(\n",
    "            {\n",
    "                \"embed_dim\": self.embed_dim,\n",
    "                \"latent_dim\": self.latent_dim,\n",
    "                \"num_heads\": self.num_heads,\n",
    "            }\n",
    "        )\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "9c66f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"encoder_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, input_vocab, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, latent_dim, num_heads, num_layers, dropout_rate)(x)\n",
    "encoder = keras.Model(encoder_inputs, encoder_outputs)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"decoder_inputs\")\n",
    "encoded_seq_inputs = keras.Input(shape=(None, embed_dim), name=\"decoder_state_inputs\")\n",
    "x = PositionalEmbedding(sequence_length, output_vocab, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, latent_dim, num_heads, num_layers, dropout_rate)(x, encoded_seq_inputs)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "decoder_outputs = tf.keras.layers.Dense(output_vocab, activation=\"softmax\")(x)\n",
    "decoder = keras.Model([decoder_inputs, encoded_seq_inputs], decoder_outputs)\n",
    "\n",
    "decoder_outputs = decoder([decoder_inputs, encoder_outputs])\n",
    "transformer = keras.Model(\n",
    "    [encoder_inputs, decoder_inputs], decoder_outputs, name=\"transformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5ede4650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "497e6120",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets' defined at (most recent call last):\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\nickg\\AppData\\Local\\Temp\\ipykernel_26568\\2264797021.py\", line 1, in <module>\n      x_train_tensor = en_tokenizer.tokenize(train_x['English'])\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\bert_tokenizer.py\", line 291, in tokenize\n      return self._wordpiece_tokenizer.tokenize(tokens)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 221, in tokenize\n      subword, _, _ = self.tokenize_with_offsets(input)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 280, in tokenize_with_offsets\n      wordpieces, starts, ends = self.tokenize_with_offsets(\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 298, in tokenize_with_offsets\n      gen_wordpiece_tokenizer.wordpiece_tokenize_with_offsets(\n    File \"<string>\", line 175, in wordpiece_tokenize_with_offsets\nNode: 'WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets'\nTable not initialized.\n\t [[{{node WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]]\n\nOriginal stack trace for 'WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets':\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n    app.start()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n    self.io_loop.start()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n    self._run_once()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n    handle._run()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n    await self.process_one()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n    await dispatch(*args)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n    await result\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n    reply_content = await reply_content\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n    res = shell.run_cell(\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n    result = runner(coro)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\nickg\\AppData\\Local\\Temp\\ipykernel_26568\\2264797021.py\", line 1, in <module>\n    x_train_tensor = en_tokenizer.tokenize(train_x['English'])\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\bert_tokenizer.py\", line 291, in tokenize\n    return self._wordpiece_tokenizer.tokenize(tokens)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 221, in tokenize\n    subword, _, _ = self.tokenize_with_offsets(input)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 280, in tokenize_with_offsets\n    wordpieces, starts, ends = self.tokenize_with_offsets(\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 298, in tokenize_with_offsets\n    gen_wordpiece_tokenizer.wordpiece_tokenize_with_offsets(\n  File \"<string>\", line 175, in wordpiece_tokenize_with_offsets\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3800, in _create_op_internal\n    ret = Operation(\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1378\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1378\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1361\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1362\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1454\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1452\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1453\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1454\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Table not initialized.\n\t [[{{node WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m transformer\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrmsprop\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      3\u001b[0m )\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtransformer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDataset_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDataset_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\engine\\training_v1.py:855\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    854\u001b[0m func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_training_loop(x)\n\u001b[1;32m--> 855\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    864\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_data\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    866\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m    \u001b[49m\u001b[43minitial_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_queue_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_queue_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mworkers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_multiprocessing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_multiprocessing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\engine\\training_arrays_v1.py:698\u001b[0m, in \u001b[0;36mArrayLikeTrainingLoop.fit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    676\u001b[0m     model,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    693\u001b[0m ):\n\u001b[0;32m    694\u001b[0m     batch_size \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_validate_or_infer_batch_size(\n\u001b[0;32m    695\u001b[0m         batch_size, steps_per_epoch, x\n\u001b[0;32m    696\u001b[0m     )\n\u001b[1;32m--> 698\u001b[0m     x, y, sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_user_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    703\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    704\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msteps_per_epoch\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    707\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_split\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    708\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m validation_data:\n\u001b[0;32m    712\u001b[0m         val_x, val_y, val_sample_weights \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39m_prepare_validation_data(\n\u001b[0;32m    713\u001b[0m             validation_data, batch_size, validation_steps\n\u001b[0;32m    714\u001b[0m         )\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\engine\\training_v1.py:2582\u001b[0m, in \u001b[0;36mModel._standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split, shuffle, extract_tensors_from_dataset)\u001b[0m\n\u001b[0;32m   2578\u001b[0m training_utils_v1\u001b[38;5;241m.\u001b[39mvalidate_dataset_input(\n\u001b[0;32m   2579\u001b[0m     x, y, sample_weight, validation_split\n\u001b[0;32m   2580\u001b[0m )\n\u001b[0;32m   2581\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m-> 2582\u001b[0m     \u001b[43mtraining_utils_v1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverify_dataset_shuffled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2584\u001b[0m is_dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extract_tensors_from_dataset:\n\u001b[0;32m   2586\u001b[0m     \u001b[38;5;66;03m# We do this for `train_on_batch`/etc.\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:1789\u001b[0m, in \u001b[0;36mverify_dataset_shuffled\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Verifies that the dataset is shuffled.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \n\u001b[0;32m   1782\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1786\u001b[0m \u001b[38;5;124;03m  boolean, whether the input dataset is shuffled or not.\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataset)\n\u001b[1;32m-> 1789\u001b[0m graph_def \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_graph_def\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m graph_def\u001b[38;5;241m.\u001b[39mnode:\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m node\u001b[38;5;241m.\u001b[39mop\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShuffleDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\engine\\training_utils_v1.py:1775\u001b[0m, in \u001b[0;36mget_dataset_graph_def\u001b[1;34m(dataset)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     graph_def_str \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39m_as_serialized_graph()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     graph_def_str \u001b[38;5;241m=\u001b[39m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_as_serialized_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1776\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mGraphDef()\u001b[38;5;241m.\u001b[39mFromString(graph_def_str)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\keras\\backend.py:4221\u001b[0m, in \u001b[0;36mget_value\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   4218\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m   4220\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m x\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mas_default():\n\u001b[1;32m-> 4221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43msession\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:993\u001b[0m, in \u001b[0;36mTensor.eval\u001b[1;34m(self, feed_dict, session)\u001b[0m\n\u001b[0;32m    969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval\u001b[39m(\u001b[38;5;28mself\u001b[39m, feed_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, session\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    970\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Evaluates this tensor in a `Session`.\u001b[39;00m\n\u001b[0;32m    971\u001b[0m \n\u001b[0;32m    972\u001b[0m \u001b[38;5;124;03m  Note: If you are not using `compat.v1` libraries, you should not need this,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    991\u001b[0m \u001b[38;5;124;03m    A numpy array corresponding to the value of this tensor.\u001b[39;00m\n\u001b[0;32m    992\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 993\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_eval_using_default_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgraph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:5792\u001b[0m, in \u001b[0;36m_eval_using_default_session\u001b[1;34m(tensors, feed_dict, graph, session)\u001b[0m\n\u001b[0;32m   5788\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m session\u001b[38;5;241m.\u001b[39mgraph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m graph:\n\u001b[0;32m   5789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot use the given session to evaluate tensor: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5790\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe tensor\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms graph is different from the session\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5791\u001b[0m                      \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgraph.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 5792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:968\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    967\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 968\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    970\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    971\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1191\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1189\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1191\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1192\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1193\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1371\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1368\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1371\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32m~\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1397\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1392\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124monly supports NHWC tensor format\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m message:\n\u001b[0;32m   1393\u001b[0m   message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mA possible workaround: Try disabling Grappler optimizer\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1394\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mby modifying the config for creating the session eg.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1395\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124msession_config.graph_options.rewrite_options.\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   1396\u001b[0m               \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisable_meta_optimizer = True\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1397\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(e)(node_def, op, message)\n",
      "\u001b[1;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets' defined at (most recent call last):\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 197, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n      app.start()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n      self.io_loop.start()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n      self._run_once()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n      handle._run()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n      result = self._run_cell(\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n      result = runner(coro)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"C:\\Users\\nickg\\AppData\\Local\\Temp\\ipykernel_26568\\2264797021.py\", line 1, in <module>\n      x_train_tensor = en_tokenizer.tokenize(train_x['English'])\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\bert_tokenizer.py\", line 291, in tokenize\n      return self._wordpiece_tokenizer.tokenize(tokens)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 221, in tokenize\n      subword, _, _ = self.tokenize_with_offsets(input)\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 280, in tokenize_with_offsets\n      wordpieces, starts, ends = self.tokenize_with_offsets(\n    File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 298, in tokenize_with_offsets\n      gen_wordpiece_tokenizer.wordpiece_tokenize_with_offsets(\n    File \"<string>\", line 175, in wordpiece_tokenize_with_offsets\nNode: 'WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets'\nTable not initialized.\n\t [[{{node WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets}}]]\n\nOriginal stack trace for 'WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets/WordpieceTokenizeWithOffsets':\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 197, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel_launcher.py\", line 17, in <module>\n    app.launch_new_instance()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\traitlets\\config\\application.py\", line 992, in launch_instance\n    app.start()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 736, in start\n    self.io_loop.start()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 195, in start\n    self.asyncio_loop.run_forever()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 601, in run_forever\n    self._run_once()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\base_events.py\", line 1905, in _run_once\n    handle._run()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\asyncio\\events.py\", line 80, in _run\n    self._context.run(self._callback, *self._args)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 516, in dispatch_queue\n    await self.process_one()\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 505, in process_one\n    await dispatch(*args)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 412, in dispatch_shell\n    await result\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 740, in execute_request\n    reply_content = await reply_content\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 422, in do_execute\n    res = shell.run_cell(\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 546, in run_cell\n    return super().run_cell(*args, **kwargs)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3024, in run_cell\n    result = self._run_cell(\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3079, in _run_cell\n    result = runner(coro)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 129, in _pseudo_sync_runner\n    coro.send(None)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3284, in run_cell_async\n    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3466, in run_ast_nodes\n    if await self.run_code(code, result, async_=asy):\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3526, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"C:\\Users\\nickg\\AppData\\Local\\Temp\\ipykernel_26568\\2264797021.py\", line 1, in <module>\n    x_train_tensor = en_tokenizer.tokenize(train_x['English'])\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\bert_tokenizer.py\", line 291, in tokenize\n    return self._wordpiece_tokenizer.tokenize(tokens)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 221, in tokenize\n    subword, _, _ = self.tokenize_with_offsets(input)\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 280, in tokenize_with_offsets\n    wordpieces, starts, ends = self.tokenize_with_offsets(\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow_text\\python\\ops\\wordpiece_tokenizer.py\", line 298, in tokenize_with_offsets\n    gen_wordpiece_tokenizer.wordpiece_tokenize_with_offsets(\n  File \"<string>\", line 175, in wordpiece_tokenize_with_offsets\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py\", line 797, in _apply_op_helper\n    op = g._create_op_internal(op_type_name, inputs, dtypes=None,\n  File \"C:\\Users\\nickg\\anaconda4\\envs\\Env39Copy\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 3800, in _create_op_internal\n    ret = Operation(\n"
     ]
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    \"rmsprop\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(Dataset_train, epochs=epochs, validation_data=Dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c78883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = Input(shape=(max_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c112280d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = open(\"textvectorvocap.txt\", \"r\", encoding=\"utf-8\")\n",
    "#m = r.readline().split(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702e264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TLayer = TextVectorization(output_sequence_length = max_padding_size, vocabulary = m, output_mode = \"int\")\n",
    "#inp = TextVectorization(output_sequence_length = max_padding_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d52056",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp = TLayer(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c040a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba2fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#inp.adapt(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee294dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495d0ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inp.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b917cd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb6d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#f = open(\"textvectorvocap.txt\", \"w\", encoding=\"utf-8\")\n",
    "#f.write(\"*\".join(inp.get_vocabulary()))\n",
    "#f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff44ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#r = open(\"textvectorvocap.txt\", \"r\", encoding=\"utf-8\")\n",
    "#m = r.readline().split(\"*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb542f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(m == inp.get_vocabulary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e06a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ae4118",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(inp.vocabulary_size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0637f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_x_vec = inp.predict(train_x)\n",
    "#test_x_vec = inp.predict(test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bb6946",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(i.__call__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f496769c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#em = Embedding(input_dim = max_length, output_dim = 512)(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5361145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#posen = keras_nlp.layers.SinePositionEncoding()(em)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e9e404",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mh1 = MultiHeadAttention(num_heads=3, key_dim=512, value_dim=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b3e529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mho1 = mh1(posen, posen, posen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8c5c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#no1 = LayerNormalization()\n",
    "#ou1 = no1(mho1+posen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4b0c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fe1 = Dense(512)(ou1)\n",
    "#ou2 = no1(ou1+fe1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a818784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lin = Dense(tar_vocab_length)(ou2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb22ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#s = Softmax()(lin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09597290",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Model(inputs=i, outputs=s)\n",
    "#print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a8e81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Need to figure out metrics for language translation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
